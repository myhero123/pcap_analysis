{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import time\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import sklearn.metrics as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = ['BENIGN', 'Bot', 'DDos', 'GlodenEye', 'Dos Hulk',\n",
    "         'Slowhttp', 'SSH', 'FTP', 'PortScan', 'slowloris', 'BruteForce', 'XSS']\n",
    "columns = ['Destination_Port', 'Flow_Duration', 'Total_Fwd_Packets',\n",
    "           'Total_Backward_Packets', 'Total_Length_of_Fwd_Packets',\n",
    "           'Total_Length_of_Bwd_Packets', 'Fwd_Packet_Length_Max',\n",
    "           'Fwd_Packet_Length_Min', 'Fwd_Packet_Length_Mean',\n",
    "           'Fwd_Packet_Length_Std', 'Bwd_Packet_Length_Max',\n",
    "           'Bwd_Packet_Length_Min', 'Bwd_Packet_Length_Mean',\n",
    "           'Bwd_Packet_Length_Std', 'Flow_Bytes/s', 'Flow_Packets/s',\n",
    "           'Flow_IAT_Mean', 'Flow_IAT_Std', 'Flow_IAT_Max', 'Flow_IAT_Min',\n",
    "           'Fwd_IAT_Total', 'Fwd_IAT_Mean', 'Fwd_IAT_Std', 'Fwd_IAT_Max',\n",
    "           'Fwd_IAT_Min', 'Bwd_IAT_Total', 'Bwd_IAT_Mean', 'Bwd_IAT_Std',\n",
    "           'Bwd_IAT_Max', 'Bwd_IAT_Min', 'Fwd_PSH_Flags', 'Bwd_PSH_Flags',\n",
    "           'Fwd_URG_Flags', 'Bwd_URG_Flags', 'Fwd_Header_Length',\n",
    "           'Bwd_Header_Length', 'Fwd_Packets/s', 'Bwd_Packets/s',\n",
    "           'Min_Packet_Length', 'Max_Packet_Length', 'Packet_Length_Mean',\n",
    "           'Packet _Length_Std', ' Packet_Length_Variance', 'FIN_Flag_Count',\n",
    "           'SYN_Flag_Count', 'RST_Flag_Count', 'PSH_Flag_Count',\n",
    "           'ACK_Flag_Count', 'URG_Flag_Count', 'CWE_Flag_Count',\n",
    "           'ECE_Flag_Count', 'Down/Up_Ratio', 'Average_Packet_Size',\n",
    "           'Avg_Fwd_Segment_Size', 'Avg_Bwd_Segment_Size',\n",
    "           'Fwd_Header_Length.1', 'Fwd_Avg_Bytes/Bulk', '_Fwd_Avg_Packets/Bulk',\n",
    "           'Fwd_Avg_Bulk_Rate', 'Bwd_Avg_Bytes/Bulk', 'Bwd_Avg_Packets/Bulk',\n",
    "           'Bwd_Avg_Bulk_Rate', 'Subflow_Fwd_Packets', 'Subflow_Fwd_Bytes',\n",
    "           'Subflow_Bwd_Packets', 'Subflow_Bwd_Bytes', 'Init_Win_bytes_forward',\n",
    "           'Init_Win_bytes_backward', 'act_data_pkt_fwd',\n",
    "           'min_seg_size_forward', 'Active_Mean', 'Active_Std', 'Active_Max',\n",
    "           'Active Min', 'Idle_Mean', 'Idle_Std', 'Idle_Max', 'Idle_Min',\n",
    "           'Label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HandleData(path):\n",
    "    list_dir = os.listdir(path)\n",
    "    fd_data = []\n",
    "    for it in list_dir:\n",
    "        data = pd.read_csv(path + '/' + it)\n",
    "        fd_data.append(data)\n",
    "    # data=fd_data\n",
    "    # data = pd.concat([fd_data[0], fd_data[1]])\n",
    "    # if len(fd_data)>=2:\n",
    "    #     for it in range(2, len(fd_data)):\n",
    "    #         data = pd.concat([data, fd_data[it]])\n",
    "    data = data.dropna(axis=0, how='any')\n",
    "    data = data.replace(',,', np.nan, inplace=False)\n",
    "    data.replace(\"Infinity\", 0, inplace=True)\n",
    "\n",
    "    data.replace('Infinity', 0.0, inplace=True)\n",
    "    data.replace('NaN', 0.0, inplace=True)\n",
    "    data.columns=columns\n",
    "    n_row, n_col = data.shape\n",
    "    data.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "    data.dropna(inplace=True)\n",
    "    data.drop_duplicates(inplace=True)\n",
    "    data.Label[data['Label'] != 'BENIGN'] = 1\n",
    "    data.Label[data['Label'] == 'BENIGN'] = 0\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=HandleData(\"/root/data/fxg/csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train(data, decomponent=False):\n",
    "    x_columns = data.columns.drop('Label')\n",
    "    x = data[x_columns].values\n",
    "    x = normalize(x, axis=0, norm='max')\n",
    "    if decomponent:\n",
    "        pca = PCA(n_components=20)\n",
    "        x = pca.fit_transform(x)\n",
    "    dummies = pd.get_dummies(data['Label'])\n",
    "    outcomes = dummies.columns\n",
    "    print(outcomes)\n",
    "    num_classes = len(outcomes)\n",
    "    print('[traffic] 类别数:', num_classes)\n",
    "    y = dummies.values\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        x, y, test_size=0.3, random_state=20)\n",
    "\n",
    "    return x_train, y_train, x_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a=Train(data)\n",
    "data.Label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RF(train_X, train_Y, test_X, test_Y):\n",
    "    print('[RF] train...')\n",
    "    t1 = time.time()\n",
    "    rfc = RandomForestClassifier()\n",
    "    rfc.fit(train_X, train_Y)\n",
    "    Y_pred = rfc.predict(test_X)\n",
    "    acc = accuracy_score(test_Y, Y_pred)\n",
    "    t2 = time.time()\n",
    "    print('acc:', acc)\n",
    "    print('using time:', t2 - t1, 'sec')\n",
    "    matrix = sm.confusion_matrix(test_Y.argmax(axis=1), Y_pred.argmax(axis=1))\n",
    "    print(matrix)\n",
    "    report = classification_report(test_Y, Y_pred)\n",
    "    print(report)\n",
    "    print('-' * 20)\n",
    "    plot_confusion_matrix(matrix, label, True, 'RF Confusion matrix')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN(train_X, train_Y, test_X, test_Y):\n",
    "    print('[KNN] train...')\n",
    "    t1 = time.time()\n",
    "    knn = KNeighborsClassifier(n_neighbors=5)\n",
    "    model = knn.fit(train_X, train_Y)\n",
    "    y_hat = model.predict(test_X)\n",
    "    acc = accuracy_score(test_Y, y_hat)\n",
    "    t2 = time.time()\n",
    "    print('acc:', acc)\n",
    "    print('using time:', t2 - t1, 'sec')\n",
    "    matrix = sm.confusion_matrix(test_Y.argmax(axis=1), y_hat.argmax(axis=1))\n",
    "    print(matrix)\n",
    "    report = classification_report(test_Y, y_hat)\n",
    "    print(report)\n",
    "    print('-' * 20)\n",
    "    # plot_confusion_matrix(matrix, label, True, 'KNN Confusion matrix')\n",
    "    return report,y_hat,acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a=KNN(Train(data))\n",
    "train_X, train_Y, test_X, test_Y = Train(data)\n",
    "KNN(train_X, train_Y, test_X, test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM(train_X, train_Y, test_X, test_Y):\n",
    "    print('[SVM] train ...')\n",
    "    train_Y = [np.where(r == 1)[0][0] for r in train_Y]\n",
    "    test_Y = [np.where(r == 1)[0][0] for r in test_Y]\n",
    "    t1 = time.time()\n",
    "    clf = svm.SVC(decision_function_shape='ovr', max_iter=300, kernel='rbf')\n",
    "    model = clf.fit(train_X, train_Y)\n",
    "    y_hat = model.predict(test_X)\n",
    "    acc = accuracy_score(test_Y, y_hat)\n",
    "    t2 = time.time()\n",
    "    print('acc:', acc)\n",
    "    print('using time:', t2 - t1, 'sec')\n",
    "    matrix = sm.confusion_matrix(test_Y, y_hat)\n",
    "    print(matrix)\n",
    "    report = classification_report(test_Y, y_hat)\n",
    "    print(report)\n",
    "    print('-' * 20)\n",
    "    plot_confusion_matrix(matrix, label, True, 'SVM Confusion matrix')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NaiveBayes(train_X, train_Y, test_X, test_Y):\n",
    "    print('[Naive Bayes] train ...')\n",
    "    train_Y = [np.where(r == 1)[0][0] for r in train_Y]\n",
    "    test_Y = [np.where(r == 1)[0][0] for r in test_Y]\n",
    "    t1 = time.time()\n",
    "    clf = BernoulliNB()\n",
    "    model = clf.fit(train_X, train_Y)\n",
    "    y_hat = model.predict(test_X)\n",
    "    acc = accuracy_score(test_Y, y_hat)\n",
    "    t2 = time.time()\n",
    "    print('acc:', acc)\n",
    "    print('using time:', t2 - t1, 'sec')\n",
    "    matrix = sm.confusion_matrix(test_Y, y_hat)\n",
    "    print(matrix)\n",
    "    report = classification_report(test_Y, y_hat)\n",
    "    print(report)\n",
    "    print('-' * 20)\n",
    "    plot_confusion_matrix(matrix, label, True, 'NB Confusion matrix')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP(train_X, train_Y, test_X, test_Y):\n",
    "    print('[MLP] train ...')\n",
    "    t1 = time.time()\n",
    "    model = MLPClassifier(hidden_layer_sizes=(100,),\n",
    "                          activation='logistic',\n",
    "                          solver='adam',\n",
    "                          learning_rate_init=0.0001,\n",
    "                          max_iter=2000)\n",
    "    model.fit(train_X, train_Y)\n",
    "    y_hat = model.predict(test_X)\n",
    "    acc = accuracy_score(test_Y, y_hat)\n",
    "    t2 = time.time()\n",
    "    print('acc:', acc)\n",
    "    print('using time:', t2 - t1, 'sec')\n",
    "    matrix = sm.confusion_matrix(test_Y.argmax(axis=1), y_hat.argmax(axis=1))\n",
    "    print(matrix)\n",
    "    report = classification_report(test_Y, y_hat)\n",
    "    print(report)\n",
    "    print('-' * 20)\n",
    "    plot_confusion_matrix(matrix, label, True, 'MLP Confusion matrix')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DT(train_X, train_Y, test_X, test_Y):\n",
    "    t1 = time.time()\n",
    "    clf = DecisionTreeClassifier(max_depth=6)\n",
    "    model = clf.fit(train_X, train_Y)\n",
    "    y_hat = model.predict(test_X)\n",
    "    acc = accuracy_score(test_Y, y_hat)\n",
    "    t2 = time.time()\n",
    "    print('acc:', acc)\n",
    "    print('using time:', t2 - t1, 'sec')\n",
    "    matrix = sm.confusion_matrix(test_Y.argmax(axis=1), y_hat.argmax(axis=1))\n",
    "    print(matrix)\n",
    "    report = classification_report(test_Y, y_hat)\n",
    "    print(report)\n",
    "    print('-' * 20)\n",
    "    plot_confusion_matrix(matrix, label, True, 'DT Confusion matrix')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DNN(train_X, train_Y, test_X, test_Y):\n",
    "    t1 = time.time()\n",
    "    model = Sequential()\n",
    "    model.add(Dense(\n",
    "        16, input_dim=train_X.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(64, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(16, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    model.add(Dense(train_Y.shape[1], activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    model.summary()\n",
    "    model.fit(train_X, train_Y, epochs=40, verbose=2)\n",
    "    y_hat = model.predict(test_X)\n",
    "    acc = accuracy_score(test_Y.argmax(axis=1), y_hat.argmax(axis=1))\n",
    "    t2 = time.time()\n",
    "    print('acc:', acc)\n",
    "    print('using time:', t2 - t1, 'sec')\n",
    "    matrix = sm.confusion_matrix(test_Y.argmax(axis=1), y_hat.argmax(axis=1))\n",
    "    print(matrix)\n",
    "    report = classification_report(test_Y.argmax(axis=1), y_hat.argmax(axis=1))\n",
    "    print(report)\n",
    "    print('-' * 20)\n",
    "    plot_confusion_matrix(matrix, label, True, 'DNN Confusion matrix')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN(train_X, train_Y, test_X, test_Y):\n",
    "    print('[CNN] train ...')\n",
    "    train_X, test_X = ImageHandle(train_X, test_X, test_Y)\n",
    "    t1 = time.time()\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv2D(\n",
    "            32, (3, 3), activation='relu', input_shape=(10, 10, 1)),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Conv2D(64, (2, 2), activation='relu'),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(test_Y.shape[1])\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                  loss=tf.keras.losses.CategoricalCrossentropy(\n",
    "                      from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    model.save('model.h5')\n",
    "    his = model.fit(train_X, train_Y, batch_size=128,\n",
    "                    verbose=2, epochs=30, validation_split=0.1)\n",
    "    print(his.history)\n",
    "    y_hat = model.predict(test_X)\n",
    "    acc = accuracy_score(test_Y.argmax(axis=1), y_hat.argmax(axis=1))\n",
    "    t2 = time.time()\n",
    "    print('acc:', acc)\n",
    "    print('using time:', t2 - t1, 'sec')\n",
    "    matrix = sm.confusion_matrix(test_Y.argmax(axis=1), y_hat.argmax(axis=1))\n",
    "    print(matrix)\n",
    "    report = classification_report(test_Y.argmax(axis=1), y_hat.argmax(axis=1))\n",
    "    print(report)\n",
    "    print('-' * 20)\n",
    "    plot_confusion_matrix(matrix, label, True, 'CNN Confusion matrix')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    data = HandleData('./input')\n",
    "    train_X, train_Y, test_X, test_Y = Train(data)\n",
    "    # RF(train_X, train_Y, test_X, test_Y)\n",
    "    # KNN(train_X, train_Y, test_X, test_Y)\n",
    "    # SVM(train_X, train_Y, test_X, test_Y)\n",
    "    # MLP(train_X, train_Y, test_X, test_Y)\n",
    "    # NaiveBayes(train_X, train_Y, test_X, test_Y)\n",
    "    # DT(train_X, train_Y, test_X, test_Y)\n",
    "    # DNN(train_X, train_Y, test_X, test_Y)\n",
    "    # CNN(train_X, train_Y, test_X, test_Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "df0893f56f349688326838aaeea0de204df53a132722cbd565e54b24a8fec5f6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
